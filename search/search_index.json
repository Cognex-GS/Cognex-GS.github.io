{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to COGNEX GSS Technical Blog Contact Info: alex.choi@cognex.com","title":"Home"},{"location":"#welcome-to-cognex-gss-technical-blog","text":"Contact Info: alex.choi@cognex.com","title":"Welcome to COGNEX GSS Technical Blog"},{"location":"posts/deep-learning/how-to-run-par-runtime/","text":"Written by Alex Choi, Oct. 14, 2020. Pre-requisites Visual Studio 2019 1. ParRuntimeTest.exe \uc2e4\ud589 \ud30c\uc77c Prebuilt: NAS: http://10.99.160.30:5000/ML Data/015_COI_rnd_tf/_research_result/ParRuntimeTest_20201014_v1.zip OneDrive: Link Source code: https://github.com/sualab/Par / branch: [Branch_SearchMode] Argument: evalType : [seg/cls] 0: classification, 1: segmentation. gpuIndex : [seg/cls] \uc0ac\uc6a9\ud560 gpu index. inputWidth/inputHeight : [seg/cls] seg\uc5d0\uc11c\ub294 runtime \uc2dc\uc758 patch size\ub97c \uc758\ubbf8\ud558\uace0, cls\uc5d0\uc11c\ub294 input image patch size\ub97c \uc758\ubbf8\ud568. patchSize : [seg] seg\uc5d0\uc11c \ucd94\ucd9c\ud560 patch size. Seg \uacb0\uacfc\ub97c cls\uc5d0\uc11c \ud65c\uc6a9\ud560 \uacbd\uc6b0 cls\uc5d0\uc11c \uc0ac\uc6a9\ud560 patch size\uac00 \uc9c0\uc815\ub418\uc5b4\uc57c \ud568. inputChannel : [seg/cls] model\uacfc input image channel. Side: 6, Front: 3. targetNumberOfPatchMin / targetNumberOfPatchMax : [seg] search mode\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uc0d8\ud50c \ub2e8\uc704 \ud328\uce58 \ucd1d \uc218\uc758 \ud3c9\uade0\uc758 \ucd5c\uc18c\uac12/\ucd5c\ub300\uac12. segThreshold / claThreshold : [seg/cls] model\uc5d0 \ub098\uc628 \ud655\ub960 \uac12\uc744 NG/OK\ub85c \uad6c\ubd84\ud558\uae30 \uc704\ud55c threshold. [0,1] \uc0ac\uc774\uc758 \ubc94\uc704 \uc9c0\uc815. 0 \ubbf8\ub9cc\uc758 \uac12\uc73c\ub85c \uc9c0\uc815 \uc2dc search mode\ub85c \ub3d9\uc791\ud568. \uc774\ub54c \uc544\ub798\uc758 probThresholdBegin\uacfc probThresholdEnd \uac12\uc774 \uc9c0\uc815\ub418\uc5b4 \uc788\uc5b4\uc57c \ud568. \ub450 \uac12\uc744 \uc9c0\uc815\ud558\uc9c0 \uc54a\uc73c\uba74 \ub450 \uac12\uc740 default \uac12\uc73c\ub85c \ub3d9\uc791\ud568. probThresholdBegin / probThresholdEnd : [seg/cls] search mode\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 threshold\uc758 search \ubc94\uc704\uc758 \ucd5c\uc18c\uac12/\ucd5c\ub300\uac12. pairSeparatorFirst / pairSeparatorSecond : [seg/cls] diffuser on/off \ub4f1 \uc774\ubbf8\uc9c0 \uc9dd\uc744 \ub9de\ucd94\uae30 \uc704\ud55c \uac12. side\uc5d0\uc11c\ub294 [\"d-off\u201d / \u201cd-on\"]\ub85c front\uc5d0\uc11c\ub294 [\u201c_Front\u201d / \u201c\u201d]\ub85c \uc124\uc815\ud558\ub098 \ucd5c\uc2e0 \ud30c\uc77c \ub124\uc774\ubc0d \uaddc\uce59\uc5d0\uc11c\ub294 \ubc14\ub014 \uc218 \uc788\uc74c. targetYieldMin / targetYieldMax : [cls] search mode\uc5d0\uc11c threshold\ub97c \ucc3e\uae30 \uc704\ud574 ok-only-wild dataset\uc744 \ud65c\uc6a9\ud558\uc5ec \uacc4\uc0b0\ud558\ub294 \uc218\uc728\uc758 \ucd5c\uc18c\uac12/\ucd5c\ub300\uac12. inputFolder / maskFolder : [seg/cls] search mode\uac00 \uc544\ub2cc \uc77c\ubc18 \ubaa8\ub4dc\uc5d0\uc11c \ud544\uc694\ub85c \ud558\ub294 image/mask \ud3f4\ub354\uc758 \uacbd\ub85c. okInputImagePath / ngInputImagePath : [cls] \uc774 \ub450 \uac12\uc774 \ubaa8\ub450 \uc9c0\uc815\ub418\uc5b4 \uc788\uc73c\uba74 okInputImagePath\uc758 \uacbd\ub85c\ub97c \uc774\uc6a9\ud558\uc5ec \ubaa8\ub378\uc758 \uc218\uc728\uc744 \uacc4\uc0b0\ud558\uace0, ngInputImagePath\uc758 \uacbd\ub85c\ub97c \uc774\uc6a9\ud558\uc5ec \ubaa8\ub378\uc758 \ubbf8\uac80\ub960\uc744 \uacc4\uc0b0\ud568. segModelPath / claModelPath : [seg/cls] model path. Result: \uae30\ubcf8 \uacbd\ub85c\ub294 ParRuntimeTest.exe \ud30c\uc77c\uc758 \uc0c1\uc704 \ud3f4\ub354\uc5d0 outs \ub77c\ub294 \ud3f4\ub354\uc5d0 \uc2e4\ud589\ud55c \uc2dc\uac04\uc73c\ub85c \uacb0\uacfc\uac00 \ucd9c\ub825\ub428. 2. Segmentation model \uc2e4\ud589 \ubc29\ubc95 Model: Link or \uc5f0\uad6c\ud300\uc5d0\uac8c \uc694\uccad or Woody\ub2d8\uc5d0\uac8c \uc694\uccad Dataset: OK dataset \ubaa9\uc801: \uc0d8\ud50c \ub2e8\uc704 \uc218\uc728\uc774 \ubaa9\ud45c\uce58 \uc774\ub0b4\uc5d0 \ub4e4\ub3c4\ub85d segmentation threshold \uc124\uc815. \uacbd\ub85c: NAS[10.99.160.32:5000]/Archive-Research/999_project/015_COI_rnd_tf/a102du-\"black/red/white\"/whole/ok-only-wild \ub610\ub294 \uc5f0\uad6c\ud300\uc5d0 \uc694\uccad. NG dataset \ubaa9\uc801: \uc704\uc758 OK dataset\uc744 \ud65c\uc6a9\ud574 \uc5bb\uc740 threshold\ub85c NG dataset\uc744 \ucc98\ub9ac\ud558\uace0, \uc774\ub54c \ud328\uce58 \ub2e8\uc704 \ud639\uc740 \uc0d8\ud50c \ub2e8\uc704\uc758 NG2OK(\ubbf8\uac80) \uc22b\uc790\ub97c \uace0\ub824\ud558\uc5ec \ubbf8\uac80\uc774 \uac00\uc7a5 \uc801\uc740 \ubaa8\ub378\uc744 \uc120\uc815. \uacbd\ub85c: NAS[10.99.160.32:5000]/Archive-Research/999_project/015_COI_rnd_tf/a102du-\"black/red/white\"/whole/ng-biased-wild \ub610\ub294 \uc5f0\uad6c\ud300\uc5d0 \uc694\uccad. Argument settings --evalType 1 --targetNumberOfPatchMin 50 --targetNumberOfPatchMax 80 --segThreshold 0.534375 --probThresholdBegin 0.3 --probThresholdEnd 0.7 --pairSeparatorFirst \"d-off\" --pairSeparatorSecond \"d-on\" --inputFolder \"E:\\999_project\\015_COI_rnd_tf\\a102du-black\\whole\\ok-only-wild\\image/\" --maskFolder \"E:\\999_project\\015_COI_rnd_tf\\a102du-black\\whole\\ok-only-wild\\mask\\non-ok.2class/\" --segModelPath \"E:\\999_project\\015_COI_rnd_tf\\_research_result\\a102du_side_seg_models\\20201013/102du_black_side_seg_epoch92_resnet18_20201013.model\" --gpuIndex 0 --patchSize 128 --inputWidth 3520 --inputHeight 512 --inputChannel 6 \uc704\uc640 \uac19\uc740 \uc635\uc158\uc744 \uac16\ub294 \uacbd\uc6b0, \uc544\ub798\uc640 \uac19\uc774 \uc2e4\ud589: --evalType 1 --targetNumberOfPatchMin 50 --targetNumberOfPatchMax 80 --segThreshold 0.534375 --probThresholdBegin 0.3 --probThresholdEnd 0.7 --pairSeparatorFirst \"d-off\" --pairSeparatorSecond \"d-on\" --inputFolder \"E:\\999_project\\015_COI_rnd_tf\\a102du-black\\whole\\ok-only-wild\\image/\" --maskFolder \"E:\\999_project\\015_COI_rnd_tf\\a102du-black\\whole\\ok-only-wild\\mask\\non-ok.2class/\" --segModelPath \"E:\\999_project\\015_COI_rnd_tf\\_research_result\\a102du_side_seg_models\\20201013/102du_black_side_seg_epoch92_resnet18_20201013.model\" --gpuIndex 0 --patchSize 128 --inputWidth 3520 --inputHeight 512 --inputChannel 6 Results \uacb0\uacfc \ud328\uce58\uac00 ParRuntimeTest.exe/../outs/\uc2dc\uac04/ \uc5d0 \uc800\uc7a5\ub428. \uc774 \uacb0\uacfc\ub294 \uc544\ub798\uc758 classification model\uc744 \ud3c9\uac00\ud558\ub294\ub370 \ubc18\ub4dc\uc2dc \ud544\uc694. 3. Classification model \ud3c9\uac00 \ubc29\ubc95 Model: Link or \uc5f0\uad6c\ud300\uc5d0\uac8c \uc694\uccad or Woody\ub2d8\uc5d0\uac8c \uc694\uccad. Dataset: OK dataset: \uc704\uc758 Segmentation model \uc2e4\ud589 \ubc29\ubc95 \uc744 \ucc38\uace0\ud558\uc5ec ok-only-wild \ub370\uc774\ud130\uc14b\uc744 \ucc98\ub9ac\ud558\uace0 \ub098\uc628 \uacb0\uacfc\ubb3c\uc778 \uc774\ubbf8\uc9c0\uc758 \ud3f4\ub354\ub97c \uc9c0\uc815. NG dataset: \uc704\uc758 Segmentation model \uc2e4\ud589 \ubc29\ubc95 \uc744 \ucc38\uace0\ud558\uc5ec ng-biased-wild \ub370\uc774\ud130\uc14b\uc744 \ucc98\ub9ac\ud558\uace0 \ub098\uc628 \uacb0\uacfc\ubb3c\uc778 \uc774\ubbf8\uc9c0\uc758 \ud3f4\ub354\ub97c \uc9c0\uc815. Argument settings --evalType 0 --targetYieldMin 0.82 --targetYieldMax 0.85 --probThresholdBegin 0.2 --probThresholdEnd 0.8 --inputFolder \"\" --okInputImagePath \"E:\\999_project\\015_COI_rnd_tf\\seg patch for cls\\side\\black\\20201013\\20201013_233934_a102du_side_black_patch_ok_only/\" --ngInputImagePath \"E:\\999_project\\015_COI_rnd_tf\\seg patch for cls\\side\\black\\20201013\\20201013_232512_a102du_side_black_patch_ng_biased/\" --pairSeparatorFirst \"d-off\" --pairSeparatorSecond \"d-on\" --claModelPath \"E:\\999_project\\015_COI_rnd_tf\\_research_result\\20201013_side_cls_models\\black_model/\" --claThreshold -1 --gpuIndex 0 --inputChannel 6 \uc704\uc640 \uac19\uc740 \uc635\uc158\uc744 \uac16\ub294 \uacbd\uc6b0, \uc544\ub798\uc640 \uac19\uc774 \uc2e4\ud589: --evalType 0 --targetYieldMin 0.82 --targetYieldMax 0.85 --probThresholdBegin 0.2 --probThresholdEnd 0.8 --inputFolder \"\" --okInputImagePath \"E:\\999_project\\015_COI_rnd_tf\\seg patch for cls\\side\\black\\20201013\\20201013_233934_a102du_side_black_patch_ok_only/\" --ngInputImagePath \"E:\\999_project\\015_COI_rnd_tf\\seg patch for cls\\side\\black\\20201013\\20201013_232512_a102du_side_black_patch_ng_biased/\" --pairSeparatorFirst \"d-off\" --pairSeparatorSecond \"d-on\" --claModelPath \"E:\\999_project\\015_COI_rnd_tf\\_research_result\\20201013_side_cls_models\\black_model/\" --claThreshold -1 --gpuIndex 0 --inputChannel 6 Results: outs \ud3f4\ub354\uc5d0 cls_result.txt \ud30c\uc77c\uc5d0 \uae30\ub85d\ub428. Classfication Results","title":"How to Run PAR Runtime"},{"location":"posts/deep-learning/how-to-run-par-runtime/#pre-requisites","text":"Visual Studio 2019","title":"Pre-requisites"},{"location":"posts/deep-learning/how-to-run-par-runtime/#1-parruntimetestexe","text":"Prebuilt: NAS: http://10.99.160.30:5000/ML Data/015_COI_rnd_tf/_research_result/ParRuntimeTest_20201014_v1.zip OneDrive: Link Source code: https://github.com/sualab/Par / branch: [Branch_SearchMode] Argument: evalType : [seg/cls] 0: classification, 1: segmentation. gpuIndex : [seg/cls] \uc0ac\uc6a9\ud560 gpu index. inputWidth/inputHeight : [seg/cls] seg\uc5d0\uc11c\ub294 runtime \uc2dc\uc758 patch size\ub97c \uc758\ubbf8\ud558\uace0, cls\uc5d0\uc11c\ub294 input image patch size\ub97c \uc758\ubbf8\ud568. patchSize : [seg] seg\uc5d0\uc11c \ucd94\ucd9c\ud560 patch size. Seg \uacb0\uacfc\ub97c cls\uc5d0\uc11c \ud65c\uc6a9\ud560 \uacbd\uc6b0 cls\uc5d0\uc11c \uc0ac\uc6a9\ud560 patch size\uac00 \uc9c0\uc815\ub418\uc5b4\uc57c \ud568. inputChannel : [seg/cls] model\uacfc input image channel. Side: 6, Front: 3. targetNumberOfPatchMin / targetNumberOfPatchMax : [seg] search mode\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uc0d8\ud50c \ub2e8\uc704 \ud328\uce58 \ucd1d \uc218\uc758 \ud3c9\uade0\uc758 \ucd5c\uc18c\uac12/\ucd5c\ub300\uac12. segThreshold / claThreshold : [seg/cls] model\uc5d0 \ub098\uc628 \ud655\ub960 \uac12\uc744 NG/OK\ub85c \uad6c\ubd84\ud558\uae30 \uc704\ud55c threshold. [0,1] \uc0ac\uc774\uc758 \ubc94\uc704 \uc9c0\uc815. 0 \ubbf8\ub9cc\uc758 \uac12\uc73c\ub85c \uc9c0\uc815 \uc2dc search mode\ub85c \ub3d9\uc791\ud568. \uc774\ub54c \uc544\ub798\uc758 probThresholdBegin\uacfc probThresholdEnd \uac12\uc774 \uc9c0\uc815\ub418\uc5b4 \uc788\uc5b4\uc57c \ud568. \ub450 \uac12\uc744 \uc9c0\uc815\ud558\uc9c0 \uc54a\uc73c\uba74 \ub450 \uac12\uc740 default \uac12\uc73c\ub85c \ub3d9\uc791\ud568. probThresholdBegin / probThresholdEnd : [seg/cls] search mode\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 threshold\uc758 search \ubc94\uc704\uc758 \ucd5c\uc18c\uac12/\ucd5c\ub300\uac12. pairSeparatorFirst / pairSeparatorSecond : [seg/cls] diffuser on/off \ub4f1 \uc774\ubbf8\uc9c0 \uc9dd\uc744 \ub9de\ucd94\uae30 \uc704\ud55c \uac12. side\uc5d0\uc11c\ub294 [\"d-off\u201d / \u201cd-on\"]\ub85c front\uc5d0\uc11c\ub294 [\u201c_Front\u201d / \u201c\u201d]\ub85c \uc124\uc815\ud558\ub098 \ucd5c\uc2e0 \ud30c\uc77c \ub124\uc774\ubc0d \uaddc\uce59\uc5d0\uc11c\ub294 \ubc14\ub014 \uc218 \uc788\uc74c. targetYieldMin / targetYieldMax : [cls] search mode\uc5d0\uc11c threshold\ub97c \ucc3e\uae30 \uc704\ud574 ok-only-wild dataset\uc744 \ud65c\uc6a9\ud558\uc5ec \uacc4\uc0b0\ud558\ub294 \uc218\uc728\uc758 \ucd5c\uc18c\uac12/\ucd5c\ub300\uac12. inputFolder / maskFolder : [seg/cls] search mode\uac00 \uc544\ub2cc \uc77c\ubc18 \ubaa8\ub4dc\uc5d0\uc11c \ud544\uc694\ub85c \ud558\ub294 image/mask \ud3f4\ub354\uc758 \uacbd\ub85c. okInputImagePath / ngInputImagePath : [cls] \uc774 \ub450 \uac12\uc774 \ubaa8\ub450 \uc9c0\uc815\ub418\uc5b4 \uc788\uc73c\uba74 okInputImagePath\uc758 \uacbd\ub85c\ub97c \uc774\uc6a9\ud558\uc5ec \ubaa8\ub378\uc758 \uc218\uc728\uc744 \uacc4\uc0b0\ud558\uace0, ngInputImagePath\uc758 \uacbd\ub85c\ub97c \uc774\uc6a9\ud558\uc5ec \ubaa8\ub378\uc758 \ubbf8\uac80\ub960\uc744 \uacc4\uc0b0\ud568. segModelPath / claModelPath : [seg/cls] model path. Result: \uae30\ubcf8 \uacbd\ub85c\ub294 ParRuntimeTest.exe \ud30c\uc77c\uc758 \uc0c1\uc704 \ud3f4\ub354\uc5d0 outs \ub77c\ub294 \ud3f4\ub354\uc5d0 \uc2e4\ud589\ud55c \uc2dc\uac04\uc73c\ub85c \uacb0\uacfc\uac00 \ucd9c\ub825\ub428.","title":"1. ParRuntimeTest.exe \uc2e4\ud589 \ud30c\uc77c"},{"location":"posts/deep-learning/how-to-run-par-runtime/#2-segmentation-model","text":"Model: Link or \uc5f0\uad6c\ud300\uc5d0\uac8c \uc694\uccad or Woody\ub2d8\uc5d0\uac8c \uc694\uccad Dataset: OK dataset \ubaa9\uc801: \uc0d8\ud50c \ub2e8\uc704 \uc218\uc728\uc774 \ubaa9\ud45c\uce58 \uc774\ub0b4\uc5d0 \ub4e4\ub3c4\ub85d segmentation threshold \uc124\uc815. \uacbd\ub85c: NAS[10.99.160.32:5000]/Archive-Research/999_project/015_COI_rnd_tf/a102du-\"black/red/white\"/whole/ok-only-wild \ub610\ub294 \uc5f0\uad6c\ud300\uc5d0 \uc694\uccad. NG dataset \ubaa9\uc801: \uc704\uc758 OK dataset\uc744 \ud65c\uc6a9\ud574 \uc5bb\uc740 threshold\ub85c NG dataset\uc744 \ucc98\ub9ac\ud558\uace0, \uc774\ub54c \ud328\uce58 \ub2e8\uc704 \ud639\uc740 \uc0d8\ud50c \ub2e8\uc704\uc758 NG2OK(\ubbf8\uac80) \uc22b\uc790\ub97c \uace0\ub824\ud558\uc5ec \ubbf8\uac80\uc774 \uac00\uc7a5 \uc801\uc740 \ubaa8\ub378\uc744 \uc120\uc815. \uacbd\ub85c: NAS[10.99.160.32:5000]/Archive-Research/999_project/015_COI_rnd_tf/a102du-\"black/red/white\"/whole/ng-biased-wild \ub610\ub294 \uc5f0\uad6c\ud300\uc5d0 \uc694\uccad. Argument settings --evalType 1 --targetNumberOfPatchMin 50 --targetNumberOfPatchMax 80 --segThreshold 0.534375 --probThresholdBegin 0.3 --probThresholdEnd 0.7 --pairSeparatorFirst \"d-off\" --pairSeparatorSecond \"d-on\" --inputFolder \"E:\\999_project\\015_COI_rnd_tf\\a102du-black\\whole\\ok-only-wild\\image/\" --maskFolder \"E:\\999_project\\015_COI_rnd_tf\\a102du-black\\whole\\ok-only-wild\\mask\\non-ok.2class/\" --segModelPath \"E:\\999_project\\015_COI_rnd_tf\\_research_result\\a102du_side_seg_models\\20201013/102du_black_side_seg_epoch92_resnet18_20201013.model\" --gpuIndex 0 --patchSize 128 --inputWidth 3520 --inputHeight 512 --inputChannel 6 \uc704\uc640 \uac19\uc740 \uc635\uc158\uc744 \uac16\ub294 \uacbd\uc6b0, \uc544\ub798\uc640 \uac19\uc774 \uc2e4\ud589: --evalType 1 --targetNumberOfPatchMin 50 --targetNumberOfPatchMax 80 --segThreshold 0.534375 --probThresholdBegin 0.3 --probThresholdEnd 0.7 --pairSeparatorFirst \"d-off\" --pairSeparatorSecond \"d-on\" --inputFolder \"E:\\999_project\\015_COI_rnd_tf\\a102du-black\\whole\\ok-only-wild\\image/\" --maskFolder \"E:\\999_project\\015_COI_rnd_tf\\a102du-black\\whole\\ok-only-wild\\mask\\non-ok.2class/\" --segModelPath \"E:\\999_project\\015_COI_rnd_tf\\_research_result\\a102du_side_seg_models\\20201013/102du_black_side_seg_epoch92_resnet18_20201013.model\" --gpuIndex 0 --patchSize 128 --inputWidth 3520 --inputHeight 512 --inputChannel 6 Results \uacb0\uacfc \ud328\uce58\uac00 ParRuntimeTest.exe/../outs/\uc2dc\uac04/ \uc5d0 \uc800\uc7a5\ub428. \uc774 \uacb0\uacfc\ub294 \uc544\ub798\uc758 classification model\uc744 \ud3c9\uac00\ud558\ub294\ub370 \ubc18\ub4dc\uc2dc \ud544\uc694.","title":"2.    Segmentation model \uc2e4\ud589 \ubc29\ubc95"},{"location":"posts/deep-learning/how-to-run-par-runtime/#3-classification-model","text":"Model: Link or \uc5f0\uad6c\ud300\uc5d0\uac8c \uc694\uccad or Woody\ub2d8\uc5d0\uac8c \uc694\uccad. Dataset: OK dataset: \uc704\uc758 Segmentation model \uc2e4\ud589 \ubc29\ubc95 \uc744 \ucc38\uace0\ud558\uc5ec ok-only-wild \ub370\uc774\ud130\uc14b\uc744 \ucc98\ub9ac\ud558\uace0 \ub098\uc628 \uacb0\uacfc\ubb3c\uc778 \uc774\ubbf8\uc9c0\uc758 \ud3f4\ub354\ub97c \uc9c0\uc815. NG dataset: \uc704\uc758 Segmentation model \uc2e4\ud589 \ubc29\ubc95 \uc744 \ucc38\uace0\ud558\uc5ec ng-biased-wild \ub370\uc774\ud130\uc14b\uc744 \ucc98\ub9ac\ud558\uace0 \ub098\uc628 \uacb0\uacfc\ubb3c\uc778 \uc774\ubbf8\uc9c0\uc758 \ud3f4\ub354\ub97c \uc9c0\uc815. Argument settings --evalType 0 --targetYieldMin 0.82 --targetYieldMax 0.85 --probThresholdBegin 0.2 --probThresholdEnd 0.8 --inputFolder \"\" --okInputImagePath \"E:\\999_project\\015_COI_rnd_tf\\seg patch for cls\\side\\black\\20201013\\20201013_233934_a102du_side_black_patch_ok_only/\" --ngInputImagePath \"E:\\999_project\\015_COI_rnd_tf\\seg patch for cls\\side\\black\\20201013\\20201013_232512_a102du_side_black_patch_ng_biased/\" --pairSeparatorFirst \"d-off\" --pairSeparatorSecond \"d-on\" --claModelPath \"E:\\999_project\\015_COI_rnd_tf\\_research_result\\20201013_side_cls_models\\black_model/\" --claThreshold -1 --gpuIndex 0 --inputChannel 6 \uc704\uc640 \uac19\uc740 \uc635\uc158\uc744 \uac16\ub294 \uacbd\uc6b0, \uc544\ub798\uc640 \uac19\uc774 \uc2e4\ud589: --evalType 0 --targetYieldMin 0.82 --targetYieldMax 0.85 --probThresholdBegin 0.2 --probThresholdEnd 0.8 --inputFolder \"\" --okInputImagePath \"E:\\999_project\\015_COI_rnd_tf\\seg patch for cls\\side\\black\\20201013\\20201013_233934_a102du_side_black_patch_ok_only/\" --ngInputImagePath \"E:\\999_project\\015_COI_rnd_tf\\seg patch for cls\\side\\black\\20201013\\20201013_232512_a102du_side_black_patch_ng_biased/\" --pairSeparatorFirst \"d-off\" --pairSeparatorSecond \"d-on\" --claModelPath \"E:\\999_project\\015_COI_rnd_tf\\_research_result\\20201013_side_cls_models\\black_model/\" --claThreshold -1 --gpuIndex 0 --inputChannel 6 Results: outs \ud3f4\ub354\uc5d0 cls_result.txt \ud30c\uc77c\uc5d0 \uae30\ub85d\ub428. Classfication Results","title":"3.    Classification model \ud3c9\uac00 \ubc29\ubc95"},{"location":"posts/deep-learning/setting-up-detectron2-dev-env/","text":"Written by Alex Choi, Oct. 21, 2020. Pre-requisites Git Anaconda3 or Python 3.7 Option 1: Using Anaconda Create conda environment named detectron2 (whatever you want for the env name) conda create -n detectron2 python=3.7 Activate the conda environment conda activate detectron2 Install PyTorch 1.6 with CUDA toolkit 10.1 (10.2 is also ok) python -m pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html Install fvcore python -m pip install fvcore Install pycocotools-windows python -m pip install pycocotools-windows Clone detectron.git any path you want git clone --depth 1 --branch v0.1.1 https://github.com/facebookresearch/detectron2.git Move to the cloned repository path cd detectron2 And install it! That's all python -m pip install -U .","title":"How to Set Up Detectron2 Development Environment"},{"location":"posts/deep-learning/setting-up-detectron2-dev-env/#pre-requisites","text":"Git Anaconda3 or Python 3.7","title":"Pre-requisites"},{"location":"posts/deep-learning/setting-up-detectron2-dev-env/#option-1-using-anaconda","text":"Create conda environment named detectron2 (whatever you want for the env name) conda create -n detectron2 python=3.7 Activate the conda environment conda activate detectron2 Install PyTorch 1.6 with CUDA toolkit 10.1 (10.2 is also ok) python -m pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html Install fvcore python -m pip install fvcore Install pycocotools-windows python -m pip install pycocotools-windows Clone detectron.git any path you want git clone --depth 1 --branch v0.1.1 https://github.com/facebookresearch/detectron2.git Move to the cloned repository path cd detectron2 And install it! That's all python -m pip install -U .","title":"Option 1: Using Anaconda"},{"location":"posts/deep-learning/setting-up-openmmlab-mmdetection-dev-env/","text":"Written by Alex Choi, Oct. 25, 2020. Introduction MMDetection is an open source object detection toolbox based on PyTorch. It is a part of the OpenMMLab project developed by Multimedia Laboratory, CUHK. Useful Links OpenMMLab MMDetection Github Page MMDetection Documentation Pre-requisites Git Anaconda3 or Python 3.7 Create conda environment and activate it Create conda environment named open-mmlab with python 3.7. conda create -n open-mmlab python=3.7 -y FIG 1. Creating open-mmlab conda environment","title":"How to Set Up OpenMMLab/MMDetection Development Environment"},{"location":"posts/deep-learning/setting-up-openmmlab-mmdetection-dev-env/#introduction","text":"MMDetection is an open source object detection toolbox based on PyTorch. It is a part of the OpenMMLab project developed by Multimedia Laboratory, CUHK.","title":"Introduction"},{"location":"posts/deep-learning/setting-up-openmmlab-mmdetection-dev-env/#useful-links","text":"OpenMMLab MMDetection Github Page MMDetection Documentation","title":"Useful Links"},{"location":"posts/deep-learning/setting-up-openmmlab-mmdetection-dev-env/#pre-requisites","text":"Git Anaconda3 or Python 3.7","title":"Pre-requisites"},{"location":"posts/deep-learning/setting-up-openmmlab-mmdetection-dev-env/#create-conda-environment-and-activate-it","text":"Create conda environment named open-mmlab with python 3.7. conda create -n open-mmlab python=3.7 -y FIG 1. Creating open-mmlab conda environment","title":"Create conda environment and activate it"},{"location":"posts/deep-learning/setting-up-par-runtime-env/","text":"Written by Alex Choi, Oct. 13, 2020. Pre-requisites Visual Studio 2019 Git NOTE: PAR runtime build requires Visual Studio 2019(VS2019). If you don't have license of VS2019, then you should get it from IT support. Clone PAR github repository Make sure that Git is installed on your system. Then in a command line tool, type the following to clone PAR github repository: git clone https://github.com/sualab/Par.git If you have no access right to the repository, contact Kiyoung.Song@cognex.com Download OpenCV Download OpenCV Windows release from OpenCV.org Download LibTorch C++ from pytorch.org Choose the CUDA toolkit version corresponding to your CUDA toolkit environment. Downloading LibTorch C++ Download both Release version and Debug version of LibTorch. Register Environment Variables For convenience of development environment of PAR , it's very useful to register environment variables for OpenCV and LibTorch. Open Edit the system environment variables on your Windows. System property window for environment variables Register LIBTORCH_1_6_0_CUDA_10_1_PATH for LibTorch release version (Change CUDA toolkit version if necessary). Variable name: LIBTORCH_1_6_0_CUDA_10_1_PATH Variable value: {PATH_TO_LIBTORCH}\\libtorch-win-shared-with-deps-1.6.0+cu101\\libtorch\\ Register LIBTORCH_1_6_0_CUDA_10_1_PATH for LibTorch release version (Change CUDA toolkit version if necessary). Variable name: LIBTORCH_1_6_0_CUDA_10_1_PATH Variable value: {PATH_TO_LIBTORCH}\\libtorch-win-shared-with-deps-1.6.0+cu101\\libtorch\\","title":"How to Set Up PAR Runtime Development Environment"},{"location":"posts/deep-learning/setting-up-par-runtime-env/#pre-requisites","text":"Visual Studio 2019 Git NOTE: PAR runtime build requires Visual Studio 2019(VS2019). If you don't have license of VS2019, then you should get it from IT support.","title":"Pre-requisites"},{"location":"posts/deep-learning/setting-up-par-runtime-env/#clone-par-github-repository","text":"Make sure that Git is installed on your system. Then in a command line tool, type the following to clone PAR github repository: git clone https://github.com/sualab/Par.git If you have no access right to the repository, contact Kiyoung.Song@cognex.com","title":"Clone PAR github repository"},{"location":"posts/deep-learning/setting-up-par-runtime-env/#download-opencv","text":"Download OpenCV Windows release from OpenCV.org","title":"Download OpenCV"},{"location":"posts/deep-learning/setting-up-par-runtime-env/#download-libtorch-c-from-pytorchorg","text":"Choose the CUDA toolkit version corresponding to your CUDA toolkit environment. Downloading LibTorch C++ Download both Release version and Debug version of LibTorch.","title":"Download LibTorch C++ from pytorch.org"},{"location":"posts/deep-learning/setting-up-par-runtime-env/#register-environment-variables","text":"For convenience of development environment of PAR , it's very useful to register environment variables for OpenCV and LibTorch. Open Edit the system environment variables on your Windows. System property window for environment variables Register LIBTORCH_1_6_0_CUDA_10_1_PATH for LibTorch release version (Change CUDA toolkit version if necessary). Variable name: LIBTORCH_1_6_0_CUDA_10_1_PATH Variable value: {PATH_TO_LIBTORCH}\\libtorch-win-shared-with-deps-1.6.0+cu101\\libtorch\\ Register LIBTORCH_1_6_0_CUDA_10_1_PATH for LibTorch release version (Change CUDA toolkit version if necessary). Variable name: LIBTORCH_1_6_0_CUDA_10_1_PATH Variable value: {PATH_TO_LIBTORCH}\\libtorch-win-shared-with-deps-1.6.0+cu101\\libtorch\\","title":"Register Environment Variables"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/","text":"Written by Alex Choi, Oct. 8, 2020. Pre-requisites Anaconda Git Visual Studio 2017 or higher Creating CONDA Environment Run a command line tool, i.e. Anaconda Prompt and type the following command: conda create -n centernet python=3.7 NOTE: Python 3.8 not tested yet. But you can try if you want. Activate centernet Anaconda environment: conda activate centernet Installing Python Packages Install pytorch(ver.1.4) and torchvision based on your cudatoolkit version. conda install pytorch=1.4 torchvision cudatoolkit=10.1 -c pytorch NOTE: CenterNet build didn't work with pytorch version 1.5 or 1.6. NOTE: cudatoolkit version 10.2 not tested yet. Install necessary python packages: python -m pip install opencv-python Cython numba progress matplotlib easydict scipy Downloading CenterNet.zip from NAS Navigate to the following path to download CenterNet.zip from NAS. \\\\10.99.160.32\\COI_images\\Research\\GSS\\CenterNet.zip Building cocoapi Tools Clone cocoapi tools git repository to any path where you want git clone https://github.com/cocodataset/cocoapi.git Open cocoapi/PythonAPI/setup.py using a text editor and modify the following line: extra_compile_args=['-Wno-cpp', '-Wno-unused-function', '-std=c99'] to extra_compile_args={'gcc': ['/Qstd=c99']}, In the command line tool move to the following path: cd cocoapi/PythonAPI Build cocoapi tools: python setup.py build_ext install Modifying cpp_extension.py Using a text editor open up C:/ProgramData/Anaconda3/envs/centernet/Lib/site-packages/torch/utils/cpp_extension.py and modify the following line: match = re.search(r'(\\d+)\\.(\\d+)\\.(\\d+)', compiler_info.decode().strip()) to match = re.search(r'(\\d+)\\.(\\d+)\\.(\\d+)', compiler_info.decode(\"utf8\", \"ignore\").strip()) Copying and Unzipping CenterNet.zip Copy \"CenterNet.zip\" file to any path you want (maybe COI project folder). Unzip the file using BandiZip or any unzipping software you want. NOTE: You can find more information from the PyTorch CenterNet official repo . Building NMS 'nms' is short for \"Non-Maximum Suppression.\" CenterNet does not usually use Non-Maximum Suppression, but it is sometimes useful. In order to avoid build error comment out the following line after opening CenterNet/src/lib/external/setup.py up using a text editor. #extra_compile_args=[\"-Wno-cpp\", \"-Wno-unused-function\"] NOTE: The line above may be already commented out actually. Build nms with the following command line: python setup.py build_ext --inplace NOTE: You can find more information about building nms here . Building DCNv2 Move to the following path in your CenterNet path: cd CenterNet/src/lib/models/networks/DCNv2/ Build DCNv2 using the following command line (this may take some time to finish building): python setup.py build develop NOTE: You can find more information about DCNv2 here . Test with Pre-trained Models Unzip \"ImageNet-Weights.zip\" and copy all the contained files (ImageNet pre-trained models) to C:\\Users\\{WINDOWS_USER_ACCOUNT_NAME}\\.cache\\torch\\checkpoints\\ . Unzip \"Centernet-Models.zip\" and copy all the contained files to CenterNet/models/ . Finally, test with some COI images using the following command line: python demo.py ctdet --demo ../data/COI/images/image_0001.jpg --load_model ../exp/ctdet/COI/model_best.pth python demo.py ctdet --demo ../data/COI/images/image_0002.jpg --load_model ../exp/ctdet/COI/model_best.pth python demo.py ctdet --demo ../data/COI/images/image_0003.jpg --load_model ../exp/ctdet/COI/model_best.pth Simple Trial for Training If you are using CenterNet.zip you can find the training information for COI from CenterNet/exp/ctdet/COI/ . There are 2 options for training: One is training from scratch (random initialization). python main.py ctdet --exp_id COI --batch_size 16 --lr 1.25e-4 --gpus 0 --load_model ../models/ctdet_coco_resdcn18.pth --resume False The other is resume training from pre-trained model. python main.py ctdet --exp_id COI --batch_size 16 --lr 1.25e-4 --gpus 0 --load_model ../models/ctdet_coco_resdcn18.pth --resume True Checking Training Information You can check your training information such as loss history, accuracy history and so on using TensorBoard . You can find your training information for COI from the path, CenterNet/exp/ctdet/COI/ Install tensorboard using the following command line: python -m pip install tensorboard Finally, install tensorboardX using the following command line: python -m pip install tensorboardX To open your training information in your web browser using tensorboard, type the following command line: tensorboard --logdir {CenterNet_ROOT}/exp/ctdet/COI","title":"How to Set Up PyTorch CenterNet Development Environment"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#pre-requisites","text":"Anaconda Git Visual Studio 2017 or higher","title":"Pre-requisites"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#creating-conda-environment","text":"Run a command line tool, i.e. Anaconda Prompt and type the following command: conda create -n centernet python=3.7 NOTE: Python 3.8 not tested yet. But you can try if you want. Activate centernet Anaconda environment: conda activate centernet","title":"Creating CONDA Environment"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#installing-python-packages","text":"Install pytorch(ver.1.4) and torchvision based on your cudatoolkit version. conda install pytorch=1.4 torchvision cudatoolkit=10.1 -c pytorch NOTE: CenterNet build didn't work with pytorch version 1.5 or 1.6. NOTE: cudatoolkit version 10.2 not tested yet. Install necessary python packages: python -m pip install opencv-python Cython numba progress matplotlib easydict scipy","title":"Installing Python Packages"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#downloading-centernetzip-from-nas","text":"Navigate to the following path to download CenterNet.zip from NAS. \\\\10.99.160.32\\COI_images\\Research\\GSS\\CenterNet.zip","title":"Downloading CenterNet.zip from NAS"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#building-cocoapi-tools","text":"Clone cocoapi tools git repository to any path where you want git clone https://github.com/cocodataset/cocoapi.git Open cocoapi/PythonAPI/setup.py using a text editor and modify the following line: extra_compile_args=['-Wno-cpp', '-Wno-unused-function', '-std=c99'] to extra_compile_args={'gcc': ['/Qstd=c99']}, In the command line tool move to the following path: cd cocoapi/PythonAPI Build cocoapi tools: python setup.py build_ext install","title":"Building cocoapi Tools"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#modifying-cpp_extensionpy","text":"Using a text editor open up C:/ProgramData/Anaconda3/envs/centernet/Lib/site-packages/torch/utils/cpp_extension.py and modify the following line: match = re.search(r'(\\d+)\\.(\\d+)\\.(\\d+)', compiler_info.decode().strip()) to match = re.search(r'(\\d+)\\.(\\d+)\\.(\\d+)', compiler_info.decode(\"utf8\", \"ignore\").strip())","title":"Modifying cpp_extension.py"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#copying-and-unzipping-centernetzip","text":"Copy \"CenterNet.zip\" file to any path you want (maybe COI project folder). Unzip the file using BandiZip or any unzipping software you want. NOTE: You can find more information from the PyTorch CenterNet official repo .","title":"Copying and Unzipping CenterNet.zip"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#building-nms","text":"'nms' is short for \"Non-Maximum Suppression.\" CenterNet does not usually use Non-Maximum Suppression, but it is sometimes useful. In order to avoid build error comment out the following line after opening CenterNet/src/lib/external/setup.py up using a text editor. #extra_compile_args=[\"-Wno-cpp\", \"-Wno-unused-function\"] NOTE: The line above may be already commented out actually. Build nms with the following command line: python setup.py build_ext --inplace NOTE: You can find more information about building nms here .","title":"Building NMS"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#building-dcnv2","text":"Move to the following path in your CenterNet path: cd CenterNet/src/lib/models/networks/DCNv2/ Build DCNv2 using the following command line (this may take some time to finish building): python setup.py build develop NOTE: You can find more information about DCNv2 here .","title":"Building DCNv2"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#test-with-pre-trained-models","text":"Unzip \"ImageNet-Weights.zip\" and copy all the contained files (ImageNet pre-trained models) to C:\\Users\\{WINDOWS_USER_ACCOUNT_NAME}\\.cache\\torch\\checkpoints\\ . Unzip \"Centernet-Models.zip\" and copy all the contained files to CenterNet/models/ . Finally, test with some COI images using the following command line: python demo.py ctdet --demo ../data/COI/images/image_0001.jpg --load_model ../exp/ctdet/COI/model_best.pth python demo.py ctdet --demo ../data/COI/images/image_0002.jpg --load_model ../exp/ctdet/COI/model_best.pth python demo.py ctdet --demo ../data/COI/images/image_0003.jpg --load_model ../exp/ctdet/COI/model_best.pth","title":"Test with Pre-trained Models"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#simple-trial-for-training","text":"If you are using CenterNet.zip you can find the training information for COI from CenterNet/exp/ctdet/COI/ . There are 2 options for training: One is training from scratch (random initialization). python main.py ctdet --exp_id COI --batch_size 16 --lr 1.25e-4 --gpus 0 --load_model ../models/ctdet_coco_resdcn18.pth --resume False The other is resume training from pre-trained model. python main.py ctdet --exp_id COI --batch_size 16 --lr 1.25e-4 --gpus 0 --load_model ../models/ctdet_coco_resdcn18.pth --resume True","title":"Simple Trial for Training"},{"location":"posts/deep-learning/setting-up-pytorch-centernet-dev-env/#checking-training-information","text":"You can check your training information such as loss history, accuracy history and so on using TensorBoard . You can find your training information for COI from the path, CenterNet/exp/ctdet/COI/ Install tensorboard using the following command line: python -m pip install tensorboard Finally, install tensorboardX using the following command line: python -m pip install tensorboardX To open your training information in your web browser using tensorboard, type the following command line: tensorboard --logdir {CenterNet_ROOT}/exp/ctdet/COI","title":"Checking Training Information"}]}